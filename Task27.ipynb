{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412b26fa-af0b-409e-8683-141267124019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb8252-4430-45fb-96b8-f16f213b6c04",
   "metadata": {},
   "source": [
    "### Problem 1: You are given a SQL file link: https://drive.google.com/file/d/1WFt7B84LTHhMueoKmz8W-PRo7xXqmZf3/view?usp=share_link. Read the data by using the file and store it in a excel file. In this data, there are 3 tables named \"invoices\", \"order_leads\" and \"sales_sql\". So create 3 sheets to your excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f99ac3a-b98f-4851-bba1-2ae745fca4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing from database world on local server\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "conn = mysql.connector.connect(host=\"localhost\",user=\"root\",password=\"\",database=\"world\")\n",
    "df1 = pd.read_sql_query(\"select * from city\",conn)\n",
    "df2 = pd.read_sql_query(\"select * from country\",conn)\n",
    "df3 = pd.read_sql_query(\"select * from countrylanguage\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea716b76-18d7-4735-8a3d-00c19e0aec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting data into excel file in different sheets\n",
    "with pd.ExcelWriter(\"World.xlsx\") as writer:\n",
    "    df1.to_excel(writer,sheet_name=\"city\",index=False)\n",
    "    df2.to_excel(writer,sheet_name=\"country\",index=False)\n",
    "    df3.to_excel(writer,sheet_name=\"countrylanguage\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d0b73-58bd-40dc-9f5d-79f71f1c1cee",
   "metadata": {},
   "source": [
    "### Problem 2: Go to the site: https://rapidapi.com/wirefreethought/api/geodb-cities. From here, you have to grab the API and have to choose proper routes to get the cities of different countries. After getting the right API, hit that API and create a dataframe of all the cities that you can get by using the API. Then store the dataframe to a SQL. If you need to create an account or have to subscribe, then do that (it has free subscription but has some limitations. Use that free subscription and modify your accordingly to get all the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f2961cc-d366-4629-8ef2-45ed0362a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wikiDataId</th>\n",
       "      <th>type</th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>region</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>regionWdId</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3350606</td>\n",
       "      <td>Q24668</td>\n",
       "      <td>CITY</td>\n",
       "      <td>Aixirivall</td>\n",
       "      <td>Aixirivall</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>Sant Julià de Lòria</td>\n",
       "      <td>06</td>\n",
       "      <td>Q24282</td>\n",
       "      <td>42.462450</td>\n",
       "      <td>1.502090</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3216144</td>\n",
       "      <td>Q24656</td>\n",
       "      <td>CITY</td>\n",
       "      <td>Aixovall</td>\n",
       "      <td>Aixovall</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>Sant Julià de Lòria</td>\n",
       "      <td>06</td>\n",
       "      <td>Q24282</td>\n",
       "      <td>42.476358</td>\n",
       "      <td>1.489492</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3406038</td>\n",
       "      <td>Q4699394</td>\n",
       "      <td>CITY</td>\n",
       "      <td>Aixàs</td>\n",
       "      <td>Aixàs</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>Sant Julià de Lòria</td>\n",
       "      <td>06</td>\n",
       "      <td>Q24282</td>\n",
       "      <td>42.486389</td>\n",
       "      <td>1.467222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3360277</td>\n",
       "      <td>Q24475</td>\n",
       "      <td>CITY</td>\n",
       "      <td>Ansalonga</td>\n",
       "      <td>Ansalonga</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>Ordino</td>\n",
       "      <td>05</td>\n",
       "      <td>Q24272</td>\n",
       "      <td>42.568443</td>\n",
       "      <td>1.521571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3341362</td>\n",
       "      <td>Q24551</td>\n",
       "      <td>CITY</td>\n",
       "      <td>Anyós</td>\n",
       "      <td>Anyós</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>La Massana</td>\n",
       "      <td>04</td>\n",
       "      <td>Q24276</td>\n",
       "      <td>42.534592</td>\n",
       "      <td>1.541650</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id wikiDataId  type        city        name  country countryCode  \\\n",
       "0  3350606     Q24668  CITY  Aixirivall  Aixirivall  Andorra          AD   \n",
       "1  3216144     Q24656  CITY    Aixovall    Aixovall  Andorra          AD   \n",
       "2  3406038   Q4699394  CITY       Aixàs       Aixàs  Andorra          AD   \n",
       "3  3360277     Q24475  CITY   Ansalonga   Ansalonga  Andorra          AD   \n",
       "4  3341362     Q24551  CITY       Anyós       Anyós  Andorra          AD   \n",
       "\n",
       "                region regionCode regionWdId   latitude  longitude  population  \n",
       "0  Sant Julià de Lòria         06     Q24282  42.462450   1.502090        1025  \n",
       "1  Sant Julià de Lòria         06     Q24282  42.476358   1.489492          69  \n",
       "2  Sant Julià de Lòria         06     Q24282  42.486389   1.467222           0  \n",
       "3               Ordino         05     Q24272  42.568443   1.521571           0  \n",
       "4           La Massana         04     Q24276  42.534592   1.541650        1006  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities\"\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"439f436758mshe81e47711e76d8ap1e2ed2jsnc25cfce27a46\",\n",
    "\t\"x-rapidapi-host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "}\n",
    "response = requests.get(url,headers=headers)\n",
    "data = response.json()[\"data\"]\n",
    "next_url = response.json()[\"links\"][1][\"href\"]\n",
    "count = response.json()[\"metadata\"][\"totalCount\"]\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b0b3fb8-3150-429b-9319-fdbf236b1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n = 1\n",
    "for i in range(1,count):\n",
    "    n+=1\n",
    "    headers = {\n",
    "\t\"x-rapidapi-key\": \"439f436758mshe81e47711e76d8ap1e2ed2jsnc25cfce27a46\",\n",
    "\t\"x-rapidapi-host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "    }\n",
    "    url = \"https://wft-geo-db.p.rapidapi.com\"\n",
    "    next_page = url + next_url\n",
    "    response = requests.get(next_page,headers=headers)\n",
    "    links = response.json()[\"links\"]\n",
    "    for i in links:\n",
    "        if i[\"rel\"] == \"next\":\n",
    "            next_url = i[\"href\"]\n",
    "    time.sleep(2)\n",
    "    data.extend(response.json()[\"data\"])\n",
    "    if n == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8f9ce60-146f-44ed-a623-b2d66bce03ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e8b6264-f61e-405b-9056-2d67e49ce201",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://root:@localhost/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9f7494a-d377-4487-b409-4d2a5680d6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(\"cities\",con=engine,if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860c865-31a0-479c-b394-f479336f74b7",
   "metadata": {},
   "source": [
    "### Problem 3: Go to this url: https://www.flipkart.com/search?q=smartphones. This is the url to find phones in flipkart website. You have to extract the below things:\n",
    "\n",
    "- image url of the phone\n",
    "- name of the image\n",
    "- average ratings\n",
    "- total ratings\n",
    "- total reviews\n",
    "- discounted price\n",
    "- actual price\n",
    "### Extract all the phones which are available in this website. So you have to use the pagination concept. Also after requesting every page through the url, please wait for a while (minimum 2-3 seconds), otherwise your IP address can be banned to access the flipkart website later.\n",
    "### After collecting all the data, save that in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "932f873e-e7be-459b-aa1c-460863126b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "\n",
    "headers = {\"User-Agent\":UserAgent().random}\n",
    "url = \"https://www.meesho.com/men-watches/pl/3k7\"\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=30)# Set 10-second timeout\n",
    "except requests.exceptions.RequestException as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c85fea09-5f0f-4651-ab5d-e577cd4655bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD>\n",
      "<TITLE>Access Denied</TITLE>\n",
      "</HEAD><BODY>\n",
      "<H1>Access Denied</H1>\n",
      " \n",
      "You don't have permission to access \"http&#58;&#47;&#47;www&#46;meesho&#46;com&#47;men&#45;watches&#47;pl&#47;3k7\" on this server.<P>\n",
      "Reference&#32;&#35;18&#46;76fed417&#46;1741702909&#46;9254f33\n",
      "<P>https&#58;&#47;&#47;errors&#46;edgesuite&#46;net&#47;18&#46;76fed417&#46;1741702909&#46;9254f33</P>\n",
      "</BODY>\n",
      "</HTML>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "# List of different user-agents to mimic real browsers\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),  \n",
    "    \"Referer\": \"https://www.google.com/\",  \n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "url = \"https://www.meesho.com/men-watches/pl/3k7\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)  # Check if content is loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14062a94-7950-448e-9d95-43f7030052a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<HTML><HEAD>\\n<TITLE>Access Denied</TITLE>\\n</HEAD><BODY>\\n<H1>Access Denied</H1>\\n \\nYou don\\'t have permission to access \"http&#58;&#47;&#47;www&#46;meesho&#46;com&#47;men&#45;watches&#47;pl&#47;3k7\" on this server.<P>\\nReference&#32;&#35;18&#46;25fed417&#46;1741702811&#46;737f87\\n<P>https&#58;&#47;&#47;errors&#46;edgesuite&#46;net&#47;18&#46;25fed417&#46;1741702811&#46;737f87</P>\\n</BODY>\\n</HTML>\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "671a148b-7390-47ac-8d22-2311b7c936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = response.text\n",
    "soup = BeautifulSoup(webpage,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e33b72d5-6726-48aa-8e5c-c0b7100577d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   503 - Service Unavailable Error\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <!--\n",
      "        To discuss automated access to Amazon data please contact api-services-support@amazon.com.\n",
      "        For information about migrating to our APIs refer to our Marketplace APIs at https://developer.amazonservices.in/ref=rm_5_sv, or our Product Advertising API at https://affiliate-program.amazon.in/gp/advertising/api/detail/main.html/ref=rm_5_ac for advertising use cases.\n",
      "-->\n",
      "  <center>\n",
      "   <a href=\"https://www.amazon.in/ref=cs_503_logo/\">\n",
      "    <img alt=\"Amazon.in\" border=\"0\" height=\"45\" src=\"https://images-eu.ssl-images-amazon.com/images/G/31/x-locale/communities/people/logo.gif\" width=\"200\"/>\n",
      "   </a>\n",
      "   <p align=\"center\">\n",
      "    <font face=\"Verdana,Arial,Helvetica\">\n",
      "     <font color=\"#CC6600\" size=\"+2\">\n",
      "      <b>\n",
      "       Oops!\n",
      "      </b>\n",
      "     </font>\n",
      "     <br/>\n",
      "     <b>\n",
      "      It's rush hour and traffic is piling up on that page. Please try again in a short while.\n",
      "      <br/>\n",
      "      If you were trying to place an order, it will not have been processed at this time.\n",
      "     </b>\n",
      "     <p>\n",
      "      <img alt=\"*\" border=\"0\" height=\"9\" src=\"https://images-eu.ssl-images-amazon.com/images/G/02/x-locale/common/orange-arrow.gif\" width=\"10\"/>\n",
      "      <b>\n",
      "       <a href=\"https://www.amazon.in/ref=cs_503_link/\">\n",
      "        Go to the Amazon.in home page to continue shopping\n",
      "       </a>\n",
      "      </b>\n",
      "     </p>\n",
      "    </font>\n",
      "   </p>\n",
      "  </center>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f8e978bc-7129-4801-b9f3-bc223e61d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import selenium.webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc1a6c54-4c76-4782-9f83-cf3983d36c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter\\\\flipkart'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\"D:\\Jupyter\\flipkart\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2be3ce74-fc3c-4fc8-8284-658d6feeedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "866c7fb9-dc69-4bd4-a7ad-cce200336ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(\"C:/Users/ASUS/Desktop/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#all_data = []\n",
    "for i in range(1,37):\n",
    "    driver.get(\"https://www.flipkart.com/search?sid=tyy%2C4io&otracker=CLP_Filters&p%5B%5D=facets.brand%255B%255D%3DSAMSUNG&page={}\".format(i))\n",
    "    html = driver.page_source\n",
    "    time.sleep(random.uniform(5, 15))\n",
    "    with open(f\"flipkart_data{i}.html\",\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4304a0ce-5c12-4d51-9359-beb9421d11e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML files merged successfully inside a single <body>!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_list\n",
    "merged_soup = BeautifulSoup(\"<html><head></head><body></body></html>\", \"html.parser\")\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        if soup.body:\n",
    "            merged_soup.body.extend(soup.body.contents)\n",
    "\n",
    "output_file = \"merged.html\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(str(merged_soup.prettify()))\n",
    "\n",
    "print(\"HTML files merged successfully inside a single <body>!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04c329a2-0f41-4806-8de4-f0e5deb2a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import time\n",
    "\n",
    "# s = Service(\"C:/Users/ASUS/Desktop/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# html = driver.page_source\n",
    "\n",
    "# driver.get(\"https://www.flipkart.com/search?sid=tyy%2C4io&p%5B%5D=facets.brand%255B%255D%3DSAMSUNG&page=1\")\n",
    "# time.sleep(5)\n",
    "\n",
    "# products = driver.find_elements(By.XPATH, '//div[contains(@class, \"_4rR01T\")]')\n",
    "\n",
    "# for product in products:\n",
    "#     print(product.text)  # Print product names\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe8ed5e5-edf1-4a3c-8ffe-1cb3db301bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merged.html\",encoding='utf-8') as f:\n",
    "    html = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c229449a-a2ab-4305-85df-b2073b0cbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fc1f83f-dc3f-4b78-aa66-8f323ab26d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = soup.find_all(\"div\",class_=\"tUxRFH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e5eb408b-382b-4663-8726-2af1248af6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = []\n",
    "photo_img_url = []\n",
    "star_rating = []\n",
    "rating = []\n",
    "review = []\n",
    "discount_price = []\n",
    "original_price = []\n",
    "ram = []\n",
    "display = []\n",
    "camera = []\n",
    "battery = []\n",
    "processor = []\n",
    "warranty = []\n",
    "prod = 1\n",
    "\n",
    "for i in container:\n",
    "    model_name.append(i.find(\"div\",class_=\"KzDlHZ\").text.strip())\n",
    "    photo_img_url.append(i.find(\"img\",class_=\"DByuf4\")[\"src\"])\n",
    "    try:\n",
    "        star_rating.append(i.find(\"div\",class_=\"XQDdHH\").text.strip())\n",
    "    except:\n",
    "        star_rating.append(\"0\")\n",
    "    \n",
    "    data = i.find_all(\"span\",class_=\"Wphh3N\")\n",
    "    if data == []:\n",
    "        rating.append(np.nan)\n",
    "        review.append(np.nan)\n",
    "        \n",
    "    else:\n",
    "        for j in data:\n",
    "            try:\n",
    "                text = j.text.strip()  # Get text safely\n",
    "                words = text.split()  # Split into words\n",
    "                if len(words)>=1:\n",
    "                    rating.append(words[0])\n",
    "                else:\n",
    "                    rating.append(np.nan)\n",
    "\n",
    "                if len(words) >=4:\n",
    "                    review.append(words[3])\n",
    "                else:\n",
    "                    review.append(np.nan)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                rating.append(np.nan)\n",
    "                review.append(np.nan) \n",
    "            \n",
    "    try:\n",
    "        discount_price.append(i.find(\"div\",class_=\"Nx9bqj _4b5DiR\").text.strip())\n",
    "    except:\n",
    "        discount_price.append(np.nan)\n",
    "    try:\n",
    "        original_price.append(i.find(\"div\",class_=\"yRaY8j ZYYwLA\").text.strip())\n",
    "    except:\n",
    "        original_price.append(np.nan)\n",
    "\n",
    "    li = i.find_all(\"li\",class_=\"J+igdf\")\n",
    "    spec = []\n",
    "    for i in li:\n",
    "        spec.append(i.text.strip())\n",
    "    ram.append(spec[0])\n",
    "    display.append(spec[1])\n",
    "    camera.append(spec[2])\n",
    "    battery.append(spec[3])\n",
    "    try:\n",
    "        processor.append(spec[4])\n",
    "    except:\n",
    "        processor.append(np.nan)\n",
    "    try:\n",
    "        warranty.append(spec[5])\n",
    "    except:\n",
    "        warranty.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c3f7e65a-bfeb-4392-b8a9-b87a9959890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_df = pd.DataFrame({\n",
    "    \"name\" : model_name,\n",
    "    \"photo_img_url\" : photo_img_url,\n",
    "    \"star_rating\" : star_rating,\n",
    "    \"rating\" : rating,\n",
    "    \"review\" : review,\n",
    "    \"discount_price\" : discount_price,\n",
    "    \"original_price\" : original_price,\n",
    "    \"ram\" : ram,\n",
    "    \"display\" : display,\n",
    "    \"camera\" : camera,\n",
    "    \"battery\" : battery,\n",
    "    \"processor\":processor,\n",
    "    \"warranty\" : warranty\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6dd539f8-d9ea-4de2-9ed6-86c94d9642eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_df.to_csv(\"samsung.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2175263-e330-422d-94ea-ad67f47956a7",
   "metadata": {},
   "source": [
    "## gitbub : https://github.com/Mahi1503/Webscrapping_Samsung_flipkart_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
